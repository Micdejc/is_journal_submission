{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdd9ac86-43d8-4abb-b6eb-6190f3140a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "Best parameters found (optimized for MCC + F1, weights=(0.6, 0.4)):\n",
      "  k_sim     = 0.000\n",
      "  alpha     = 0.000\n",
      "-------------------------------------------\n",
      "Representative stable parameters (±5% variation in the two chosen metrics):\n",
      "  k_sim     = 0.658\n",
      "  alpha     = 0.372\n",
      "===========================================\n",
      "\n",
      "Dynamic similarity threshold: 0.810, Average sim: 0.877\n",
      "\n",
      "✅ Semantic jailbreaking evaluation complete for model all-MiniLM-L6-v2! Results saved to: tests\\all-MiniLM-L6-v2\\gemini-2.0-flash_attack_N_1_past_semantic_similarities_evaluation.csv\n",
      "\n",
      "Success vs Human Metrics:\n",
      "  MCC: 0.018\n",
      "  Cohen's Kappa: 0.017\n",
      "  Accuracy: 0.551\n",
      "  Precision: 0.245\n",
      "  Recall: 0.433\n",
      "  F1-score: 0.313\n",
      "  Pearson's Correlation (r): 0.018\n",
      "  P-value: 8.403e-01\n",
      "\n",
      "llama-guard-3-8b Judge vs Human Metrics:\n",
      "  MCC: -0.050\n",
      "  Cohen's Kappa: -0.015\n",
      "  Accuracy: 0.756\n",
      "  Precision: 0.000\n",
      "  Recall: 0.000\n",
      "  F1-score: 0.000\n",
      "  Pearson's Correlation (r): -0.050\n",
      "  P-value: 5.802e-01\n",
      "\n",
      "gpt-4.1 Judge vs Human Metrics:\n",
      "  MCC: 0.349\n",
      "  Cohen's Kappa: 0.290\n",
      "  Accuracy: 0.638\n",
      "  Precision: 0.379\n",
      "  Recall: 0.833\n",
      "  F1-score: 0.521\n",
      "  Pearson's Correlation (r): 0.349\n",
      "  P-value: 5.748e-05\n",
      "\n",
      "is_jailbreak_sim_all-MiniLM-L6-v2 vs Human Metrics:\n",
      "  MCC: 0.782\n",
      "  Cohen's Kappa: 0.782\n",
      "  Accuracy: 0.921\n",
      "  Precision: 0.833\n",
      "  Recall: 0.833\n",
      "  F1-score: 0.833\n",
      "  Pearson's Correlation (r): 0.782\n",
      "  P-value: 2.070e-27\n"
     ]
    }
   ],
   "source": [
    "# run_evaluator\n",
    "\n",
    "import importlib\n",
    "\n",
    "# Import the module (not the class!)\n",
    "import multiturn_evaluator\n",
    "\n",
    "# Reload the module (if needed)\n",
    "importlib.reload(multiturn_evaluator)\n",
    "\n",
    "# Now import or use the class\n",
    "from multiturn_evaluator import MultiTurnEvaluator\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Create an instance of the moderator\n",
    "    evaluator = MultiTurnEvaluator()\n",
    "\n",
    "    # Variable for number of iterations in moderation\n",
    "    nb_iter = 1\n",
    "\n",
    "    # Variable for past tense reformulation in moderation\n",
    "    is_past = True\n",
    "\n",
    "    # Variable to specify if training or not\n",
    "    is_training = False\n",
    "\n",
    "    # Optional variables for the row range (human level) to select from the input file\n",
    "    min_row = None\n",
    "    max_row = None\n",
    "\n",
    "    # Variable to control the targeted LLM for moderation\n",
    "    #Is the targeted model open or close\n",
    "    is_targeted_llm_open = False\n",
    "    \n",
    "    # Index in the list of the targeted LLM\n",
    "    # Refer to LLM lists in LLMAggregator Class\n",
    "    targeted_llm_index = 1\n",
    "\n",
    "    # Extract human refusal set from LLM responses\n",
    "    #evaluator.human_moderation_extraction_embedding(is_targeted_llm_open, targeted_llm_index, nb_iter, is_past, min_row, max_row)\n",
    "\n",
    "    # Calculate semantic distances for LLM responses set\n",
    "    #evaluator.llm_reponse_semantic_similarities(is_targeted_llm_open, targeted_llm_index, nb_iter, is_past, is_training, min_row, max_row)\n",
    "\n",
    "    # Assess jailbreaking using semantic similarities for LLM responses set\n",
    "    evaluator.llm_reponse_semantic_evaluation(is_targeted_llm_open, targeted_llm_index, nb_iter, is_past, min_row, max_row)\n",
    "\n",
    "    # Calculate evaluation mertics for LLM responses set for stats\n",
    "    #evaluator.calculate_evaluation_metrics(is_targeted_llm_open, targeted_llm_index, nb_iter, is_past)\n",
    "\n",
    "    # Calculate evaluation mertics per cyber subtopic for LLM responses set for stats\n",
    "    #evaluator.calculate_evaluation_metrics_per_cybersubtopic(is_targeted_llm_open, targeted_llm_index, nb_iter, is_past)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ee3ae-b0eb-4597-9471-69add8229e93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
